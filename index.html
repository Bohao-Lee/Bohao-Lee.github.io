<!DOCTYPE html>

<HTML>
<HEAD>
  <META content="IE=5.0000" http-equiv="X-UA-Compatible">
  <META name="description" content="Bohao Li's home page"> 
  <META http-equiv="Content-Type" content="text/html; charset=gb2312">
  <LINK href="files/doc.css" 
    rel="stylesheet" type="text/css"> 
  <TITLE>Bohao Li</TITLE> 
  <META name="GENERATOR" content="MSHTML 11.00.10570.1001">
</HEAD>


<BODY> 
  <DIV id="layout-content" style="margin-top: 25px;">
  <TABLE>
    <TBODY>
    <TR>
      <TD width="670">
        <DIV id="toptitle">
        <H1>Bohao Li &nbsp;</H1></DIV>
        <H3>Ph.D. candidate</H3>
        <BR>School of Data Science
        <BR>The Chinese University of Hong Kong, Shenzhen
        <BR>Shenzhen, China, 518000.
        <BR>
        <BR> Email:  
        <A href="mailto:libohao1998@gmail.com"> libohao1998@gmail.com</A>; 
        <BR> Github: 
        <A href="https://github.com/Bohao-Lee">https://github.com/Bohao-Lee</A>;
        <BR> Google scholar:
        <A href="https://scholar.google.com/citations?user=YSuTMAIAAAAJ&hl=en">https://scholar.google.com</A>
        <BR><BR></P>
      </TD>
      <TD>
        <IMG width="150" src="files/person_photo.jpg" border="0">
      </TD>
    </TR>
    <TR></TR></TBODY>
  </TABLE>
  <DIV id="layout-content" style="margin-top: 25px;">


  <H2>Biography</H2>
  <P> I am a Ph.D. candidate in the <A href="https://sds.cuhk.edu.cn/en">School of Data Science</A>, 
    <A href="https://www.cuhk.edu.cn/en">The Chinese University of Hong Kong, Shenzhen </A>, 
    co-advised by <A href="http://www.zhangruimao.site">Prof. Ruimao Zhang</A> and <A href="https://shuangli01.github.io/index.html">Prof. Shuang Li</A>.
    I got a M.E. degree in University of Chinese Academy of Sciences, Beijing in June 2023, advised by <A href="http://people.ucas.ac.cn/~0007279?language=en">Prof. Qixiang Ye</A>. 
    I got a B.E. degree in Wuhan University, Wuhan in June 2020.
  </P>

  <P>My research interests include computer vision and deep learning, specifically for few-shot learning and multimodal.</P>


  <H2>Publications</H2>
    <table class="pub_table">
    <!-- <tbody> -->
      <tr>
        <td class="pub_td1"><img src="files/PaperFig/seed-bench-2-plus.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Bohao Li</u>, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, Ying Shan
          <br><b>SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension</b>
          <br>
          [<a href="https://arxiv.org/abs/2404.16790">Paper</a>]
          [<a href="https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2-Plus">Dataset</a>]
          [<a href="https://github.com/AILab-CVC/SEED-Bench">Code</a>]
          <img src="https://img.shields.io/github/stars/AILab-CVC/SEED-Bench?style=social">
          <br>
        </td>
      </tr>

      
      <tr>
        <td class="pub_td1"><img src="files/PaperFig/seed-bench-2.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast;<u>Bohao Li</u>, &ast;Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan
          <br><b>SEED-Bench-2: Benchmarking Multimodal Large Language Models</b>
          <br>IEEE Conference on Computer Vision and Pattern Recognition, 2024
          <br> TechBeat Community's 2024 Popular Technology Work [<a href="https://mp.weixin.qq.com/s/4KT0fAdx1hok0cEU5XcNIQ">Link</a>]
          <br>
          [<a href="https://arxiv.org/abs/2311.17092">Paper</a>]
          [<a href="https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2">Dataset</a>]
          [<a href="https://github.com/AILab-CVC/SEED-Bench">Code</a>]
          [<a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard">Leaderborad</a>]
          <img src="https://img.shields.io/github/stars/AILab-CVC/SEED-Bench?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/seed-bench.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast;<u>Bohao Li</u>, &ast;Rui Wang, &ast;Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan
          <br><b>Seed-bench: Benchmarking multimodal llms with generative comprehension</b>
          <br>
          [<a href="https://arxiv.org/abs/2307.16125">Paper</a>]
          [<a href="https://huggingface.co/datasets/AILab-CVC/SEED-Bench">Dataset</a>]
          [<a href="https://github.com/AILab-CVC/SEED-Bench">Code</a>]
          [<a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard">Leaderborad</a>]
          <img src="https://img.shields.io/github/stars/AILab-CVC/SEED-Bench?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/PDC.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Bohao Li</u>, Chang Liu, Mengnan Shi, Xiaozhong Chen, Xiangyang Ji, Qixiang Ye
          <br><b>Proposal Distribution Calibration for Few-Shot Object Detection</b>
          <br>IEEE Transactions on Neural Networks and Learning Systems, 2024
          <br>
          [<a href="https://arxiv.org/abs/2212.07618">Paper</a>]
          [<a href="https://github.com/Bohao-Lee/PDC">Code</a>]
          <img src="https://img.shields.io/github/stars/Bohao-Lee/PDC?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/CME.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast;<u>Bohao Li</u>, &ast; Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, Qixiang Ye
          <br><b>Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection</b>
          <br>IEEE Conference on Computer Vision and Pattern Recognition, 2021
          <br>
          [<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Beyond_Max-Margin_Class_Margin_Equilibrium_for_Few-Shot_Object_Detection_CVPR_2021_paper.html?ref=https://githubhelp.com">Paper</a>]
          [<a href="https://github.com/Bohao-Lee/CME">Code</a>]
          <img src="https://img.shields.io/github/stars/Bohao-Lee/CME?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/egoplan_bench.png" class="papericon"></td>
        <td 
          class="pub_td2">Chen Yi, Yuying Ge, Yixiao Ge, Mingyu Ding, <u>Bohao Li</u>, Rui Wang, Ruifeng Xu, Ying Shan, Xihui Liu
          <br><b>EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models</b>
          <br>
          [<a href="https://chenyi99.github.io/ego_plan">Project</a>]
          [<a href="https://arxiv.org/abs/2312.06722">Paper</a>]
          [<a href="https://drive.google.com/drive/folders/1OUnQzG79kxhJdaquBKLv1rrKz36TTkP6">Dataset</a>]
          [<a href="https://github.com/ChenYi99/EgoPlan">Code</a>]
          <img src="https://img.shields.io/github/stars/ChenYi99/EgoPlan?style=social">
          <br>
        </td>
      </tr>
      
      <tr>
        <td class="pub_td1"><img src="files/PaperFig/CaFo.png" class="papericon"></td>
        <td 
          class="pub_td2">Renrui Zhang, Xiangfei Hu, <u>Bohao Li</u>, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, Hongsheng Li
          <br><b>Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners</b>
          <br>IEEE Conference on Computer Vision and Pattern Recognition, 2023
          <br>
          [<a href="http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.html">Paper</a>]
          [<a href="https://github.com/OpenGVLab/CaFo">Code</a>]
          <img src="https://img.shields.io/github/stars/OpenGVLab/CaFo?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/PMMs.png" class="papericon"></td>
        <td 
          class="pub_td2">Boyu Yang, Chang Liu, <u>Bohao Li</u>, Jianbin Jiao, Qixiang Ye
          <br><b>Prototype mixture models for few-shot semantic segmentation</b>
          <br>European Conference on Computer Vision (ECCV), 2020
          <br>
          [<a href="https://link.springer.com/chapter/10.1007/978-3-030-58598-3_45">Paper</a>]
          [<a href="https://github.com/Yang-Bob/PMMs">Code</a>]
          <img src="https://img.shields.io/github/stars/Yang-Bob/PMMs?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/PST.png" class="papericon"></td>
        <td 
          class="pub_td2">Boyu Yang, Fang Wan, Chang Liu, <u>Bohao Li</u>, Xiangyang Ji, Qixiang Ye
          <br><b>Part-based semantic transform for few-shot semantic segmentation</b>
          <br>IEEE Transactions on Neural Networks and Learning Systems, 2021
          <br>
          [<a href="https://ieeexplore.ieee.org/abstract/document/9448305">Paper</a>]
          [<a href="https://github.com/Yang-Bob/PST">Code</a>]
          <img src="https://img.shields.io/github/stars/Yang-Bob/PST?style=social">
          <br>
        </td>
      </tr>
     
    <!-- </tbody> -->
    </table>

    <!-- <br>
    <H2>Awards</H2>
        <LI>	Excellent Student Scholarship, Chinese Academy of Sciences, 2020.  </LI> -->
  <H2>Github Statistics</Source></H2>
      <td class="pub_td1"><img src="https://github-readme-stats.vercel.app/api?username=Bohao-Lee&show_icons=true&include_all_commits=true&title_color=2c86ea&icon_color=2c86ea&text_color=00c800&bg_color=00000000"></td>
    
  
  <br> <br> 
  <H2>Statistics</H2>
  <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5063gq35g0n&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>

</BODY>
</HTML>
